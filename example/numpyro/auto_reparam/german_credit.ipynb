{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ec475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "import jax.numpy as np\n",
    "import pandas\n",
    "import pickle\n",
    "\n",
    "numpyro.set_platform('cpu')\n",
    "numpyro.set_host_device_count(10)\n",
    "\n",
    "def preprocess():\n",
    "    german_data_raw = pandas.read_csv('german.data',header=None,sep='\\s+')\n",
    "\n",
    "    modified_columns = []\n",
    "    column_info = {}\n",
    "    column_index = {}\n",
    "\n",
    "\n",
    "    column_end = 0\n",
    "    for i in german_data_raw:\n",
    "        raw_column = german_data_raw[i]\n",
    "        print(i, \": \", raw_column.dtype)\n",
    "\n",
    "        column_start = column_end\n",
    "\n",
    "        if raw_column.dtype == 'O':\n",
    "            categories = np.array(raw_column.map(lambda s : int(s.removeprefix(\"A\"+str(i+1)))))\n",
    "            column_info[i],indices = np.unique(categories, return_inverse=True)\n",
    "            column_length = column_info[i].shape[0]\n",
    "\n",
    "            modified_columns.append(np.eye(column_length)[indices])\n",
    "\n",
    "            column_end = column_start + column_length\n",
    "        else:\n",
    "            modified_columns.append(np.expand_dims(np.array(raw_column),1))\n",
    "            column_end = column_start + 1\n",
    "        \n",
    "        column_index[i] = (column_start,column_end)\n",
    "    return column_info, column_index, np.concatenate(modified_columns,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"load column info...\")\n",
    "    with open('data/column_info.pkl', 'rb') as f:\n",
    "        cinfo = pickle.load(f)\n",
    "    print(\"load column index...\")\n",
    "    with open('data/column_index.pkl', 'rb') as f:\n",
    "        cidx = pickle.load(f)\n",
    "    print(\"load german credit data...\")\n",
    "    german_data = np.load(\"data/german.npy\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    cinfo, cidx, german_data = preprocess()\n",
    "    np.save(\"data/german.npy\",german_data)\n",
    "    with open('data/column_info.pkl', 'wb') as f:\n",
    "        pickle.dump(cinfo, f)\n",
    "    \n",
    "    with open('data/column_index.pkl', 'wb') as f:\n",
    "        pickle.dump(cidx, f)\n",
    "\n",
    "import numpy\n",
    "\n",
    "normalized_features = numpy.array(german_data[:,:-1])\n",
    "\n",
    "mask = numpy.ones(len(cidx.keys()), dtype=bool)\n",
    "mask[numpy.fromiter(cinfo.keys(), dtype=int)]=False\n",
    "\n",
    "numeric_idx = numpy.array([cidx[i][0] for i,c in enumerate(mask[:-1]) if c])\n",
    "\n",
    "normalized_features[:,numeric_idx] = (normalized_features[:,numeric_idx] - numpy.mean(normalized_features[:,numeric_idx], axis=0))/numpy.std(normalized_features[:,numeric_idx], axis=0)\n",
    "\n",
    "normalized_features = np.array(normalized_features)\n",
    "credits = 2-german_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ceffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def german_credit(features, credits):\n",
    "    num_obs, num_feature = features.shape\n",
    "\n",
    "    log_tau_beta_global = numpyro.sample('log_tau_beta_global', dist.Normal(0, 10))\n",
    "    #alpha = numpyro.sample('alpha', dist.Normal(0, 1))\n",
    "\n",
    "    with numpyro.plate('feature', num_feature):\n",
    "        log_tau_beta = numpyro.sample('log_tau_beta',dist.Normal(log_tau_beta_global, 1))\n",
    "        beta = numpyro.sample('beta', dist.Normal(0, np.exp(log_tau_beta)))\n",
    "\n",
    "    logits = np.dot(features, beta)#+alpha\n",
    "    with numpyro.plate('observation', num_obs):\n",
    "        return numpyro.sample('credit', dist.Bernoulli(logits = logits),obs=credits)\n",
    "\n",
    "\n",
    "\n",
    "from jax import random\n",
    "from numpyro.infer import MCMC, NUTS \n",
    "\n",
    "from numpyro.handlers import reparam\n",
    "from numpyro.infer.reparam import LocScaleReparam\n",
    "\n",
    "nuts_kernel = NUTS(german_credit)\n",
    "\n",
    "mcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=5000 ,num_chains=10)\n",
    "\n",
    "rng_key = random.PRNGKey(0)\n",
    "\n",
    "mcmc.run(rng_key, features=normalized_features, credits = credits, extra_fields=('num_steps',))\n",
    "\n",
    "mcmc.print_summary()\n",
    "\n",
    "\n",
    "\n",
    "reparam_model = reparam(german_credit, config={\"log_tau_beta_global\": LocScaleReparam(0),\"log_tau_beta\": LocScaleReparam(0),\"beta\": LocScaleReparam(0)})\n",
    "\n",
    "nuts_kernel2 = NUTS(reparam_model)\n",
    "\n",
    "mcmc2 = MCMC(nuts_kernel2, num_warmup=1000, num_samples=5000 ,num_chains=10)\n",
    "\n",
    "rng_key2 = random.PRNGKey(0)\n",
    "\n",
    "mcmc2.run(rng_key, features=normalized_features, credits = credits, extra_fields=('num_steps',))\n",
    "\n",
    "\n",
    "mcmc2.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da08746",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = mcmc.get_extra_fields()\n",
    "fields2 = mcmc2.get_extra_fields()\n",
    "\n",
    "print(np.sum(fields['num_steps']), np.sum(fields2['num_steps']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09df3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "samples = mcmc.get_samples()\n",
    "samples2 = mcmc2.get_samples()\n",
    "\n",
    "plt.scatter(samples2[\"log_tau_beta_decentered\"][:,8],samples2[\"beta_decentered\"][:,8], s = 1, c = \"red\")\n",
    "plt.scatter(samples[\"log_tau_beta\"][:,8],samples[\"beta\"][:,8], s = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.infer import SVI, Trace_ELBO\n",
    "from numpyro.infer.autoguide import AutoNormal\n",
    "from numpyro.optim import Adam\n",
    "\n",
    "optimizer = Adam(step_size = 0.005)\n",
    "\n",
    "learnable_model = reparam(german_credit, config={\"log_tau_beta_global\": LocScaleReparam(),\"log_tau_beta\": LocScaleReparam(),\"beta\": LocScaleReparam()})\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(learnable_model, AutoNormal(learnable_model), optimizer, loss=Trace_ELBO())\n",
    "\n",
    "# do gradient steps\n",
    "svi_result = svi.run(random.PRNGKey(0), 20000, features=normalized_features, credits = credits)\n",
    "params = svi_result.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc59bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_kernel3 = NUTS(learnable_model)\n",
    "\n",
    "mcmc3 = MCMC(nuts_kernel3, num_warmup=1000, num_samples=5000 ,num_chains=10)\n",
    "\n",
    "rng_key3 = random.PRNGKey(0)\n",
    "\n",
    "mcmc3.run(rng_key3, features=normalized_features, credits = credits, extra_fields=('num_steps',))\n",
    "\n",
    "mcmc3.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4fbf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples3 = mcmc3.get_samples()\n",
    "plt.scatter(samples3[\"log_tau_beta\"][:,8],samples3[\"beta\"][:,8], s = 1)\n",
    "plt.scatter(samples3[\"log_tau_beta_decentered\"][:,8],samples3[\"beta_decentered\"][:,8], s = 1, c = 'red')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('numpyro')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a6483b6e6f647e6df113ab5f48f1f87a759fad2630802ebe8bd3a55121497d18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
